<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>Your Shadow Training Set: How Slack, LinkedIn, Anthropic and Others Use Your Work Data for AI | Attorney Sergei Tokmakov</title>
<meta name="description" content="Beyond Upwork: how your everyday toolsâ€”Slack, LinkedIn, Anthropic Claude, OpenAI, GitHubâ€”train AI on your work data. Privacy policy analysis, opt-out strategies, and enterprise vs consumer AI training differences.">
<style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:Georgia,serif;line-height:1.7;color:#2c3e50;background:#f8f9fa;padding:20px}header{background:linear-gradient(135deg,#dc2626,#ef4444);color:#fff;padding:30px 20px;text-align:center;border-radius:8px;margin-bottom:40px;box-shadow:0 4px 6px rgba(0,0,0,0.1)}h1{font-size:2.5em;margin-bottom:15px;font-weight:700}h2{color:#dc2626;margin:30px 0 15px;font-size:1.8em;border-bottom:3px solid #dc2626;padding-bottom:8px}h3{color:#2c3e50;margin:25px 0 12px;font-size:1.4em}h4{color:#374151;margin:20px 0 10px;font-size:1.15em}.container{max-width:1100px;margin:0 auto;background:#fff;padding:50px;border-radius:8px;box-shadow:0 2px 10px rgba(0,0,0,0.08)}p{margin-bottom:18px;font-size:1.05em;text-align:justify}ul,ol{margin:15px 0 15px 35px}li{margin-bottom:10px}.intro{font-size:1.15em;color:#374151;margin-bottom:30px;padding:25px;background:#fee2e2;border-left:5px solid #dc2626;border-radius:5px}.cta-box{background:linear-gradient(135deg,#fee2e2,#fecaca);border:2px solid #dc2626;border-radius:8px;padding:30px;margin:35px 0;text-align:center}.cta-box h3{color:#dc2626;margin-top:0}.cta-box a{display:inline-block;background:#dc2626;color:#fff;padding:14px 32px;text-decoration:none;border-radius:5px;font-weight:700;margin-top:15px;transition:background 0.3s}.cta-box a:hover{background:#ef4444}.tabs{display:flex;flex-wrap:wrap;border-bottom:3px solid #dc2626;margin:40px 0 0;gap:5px}.tab{background:#fee2e2;border:none;padding:15px 25px;cursor:pointer;font-size:1.05em;font-weight:600;color:#dc2626;transition:all 0.3s;border-radius:5px 5px 0 0;flex:1;min-width:150px;text-align:center}.tab:hover{background:#fecaca}.tab.active{background:#dc2626;color:#fff}.tab-content{display:none;padding:35px 0;animation:fadeIn 0.5s}.tab-content.active{display:block}@keyframes fadeIn{from{opacity:0}to{opacity:1}}table{width:100%;border-collapse:collapse;margin:25px 0;font-size:0.98em}th,td{border:1px solid #bdc3c7;padding:14px;text-align:left;vertical-align:top}th{background:#dc2626;color:#fff;font-weight:700}tr:nth-child(even){background:#fee2e2}.warning-box{background:#fff3cd;border-left:5px solid #f39c12;padding:20px;margin:25px 0;border-radius:5px}.warning-box h4{color:#f39c12;margin-top:0}.info-box{background:#dbeafe;border-left:5px solid #2563eb;padding:20px;margin:25px 0;border-radius:5px}.info-box h4{color:#2563eb;margin-top:0}.danger-box{background:#fee2e2;border-left:5px solid #dc2626;padding:20px;margin:25px 0;border-radius:5px}.danger-box h4{color:#dc2626;margin-top:0}.timeline{background:#f3f4f6;border-left:5px solid #dc2626;padding:25px;margin:30px 0;border-radius:8px}.timeline h4{color:#dc2626;margin-top:0}.timeline-item{margin:15px 0;padding-left:25px;border-left:3px solid #dc2626;position:relative}.timeline-item:before{content:"";position:absolute;left:-8px;top:8px;width:12px;height:12px;background:#dc2626;border-radius:50%}.timeline-item strong{color:#dc2626}.consumer-enterprise{display:grid;grid-template-columns:1fr 1fr;gap:25px;margin:30px 0}@media(max-width:768px){.consumer-enterprise{grid-template-columns:1fr}}.consumer{background:#fee2e2;padding:25px;border-radius:8px;border-left:5px solid#dc2626}.consumer h4{color:#dc2626;margin-top:0}.enterprise{background:#d1fae5;padding:25px;border-radius:8px;border-left:5px solid #10b981}.enterprise h4{color:#10b981;margin-top:0}.key-takeaway{background:#fff;border:3px solid #dc2626;border-radius:8px;padding:30px;margin:30px 0}.key-takeaway h4{color:#dc2626;margin-top:0;font-size:1.4em}@media(max-width:768px){.container{padding:25px}h1{font-size:1.9em}h2{font-size:1.5em}.tabs{flex-direction:column}.tab{min-width:100%}table{font-size:0.85em}th,td{padding:10px}}
</style>
</head>
<body>
<header>
<h1>Your Shadow Training Set</h1>
<p style="font-size:1.2em;margin-top:10px">How Slack, LinkedIn, Anthropic and Others Use Your Work Data for AI</p>
</header>

<div class="container">
<div class="intro">
<strong>You configured Upwork's AI Preferences. You banned AI on Fiverr orders.</strong> But what about Slack, where your team discusses strategy all day? Or LinkedIn, where you message prospects? Or Claude, where you paste confidential drafts? The same company that locked down marketplace AI training may be feeding work product to dozens of other toolsâ€”without realizing it.
</div>

<div class="tabs">
<button class="tab active" onclick="openTab(event,'tab1')">Overview</button>
<button class="tab" onclick="openTab(event,'tab2')">Data-Use Models</button>
<button class="tab" onclick="openTab(event,'tab3')">Major Tools Policies</button>
<button class="tab" onclick="openTab(event,'tab4')">Timeline & Cases</button>
<button class="tab" onclick="openTab(event,'tab5')">Consumer vs Enterprise</button>
<button class="tab" onclick="openTab(event,'tab6')">Building Your Policy</button>
<button class="tab" onclick="openTab(event,'tab7')">Attorney Services</button>
</div>

<div id="tab1" class="tab-content active">
<h2>Why Upwork Isn't the Whole Story</h2>

<p>Freelance platforms are just one piece of your company's data footprint. A typical knowledge-work company also routes sensitive information through:</p>

<ul>
<li><strong>Team collaboration tools:</strong> Slack, Microsoft Teams, Zoom, Google Workspace</li>
<li><strong>Professional networks:</strong> LinkedIn</li>
<li><strong>AI assistants:</strong> ChatGPT, Claude, Copilot, Gemini</li>
<li><strong>Code and documentation:</strong> GitHub, GitLab, Notion, Confluence</li>
<li><strong>Project management:</strong> Asana, Monday, ClickUp</li>
<li><strong>Customer support:</strong> Intercom, Zendesk with AI features</li>
</ul>

<p>Each of these has its own AI training policy, data retention rules, and opt-in/opt-out mechanics. And they're evolving fastâ€”what was "safe" six months ago may have changed.</p>

<h3>What This Article Covers</h3>

<p>This guide maps the current landscape across platforms:</p>

<ul>
<li>How major tools (Slack, LinkedIn, Anthropic Claude, OpenAI, GitHub, etc.) train on your data</li>
<li>Recent controversies and what they revealed about platform practices</li>
<li>Consumer vs enterprise tier differences in AI training policies</li>
<li>How to build a company-wide AI & SaaS policy that's consistent across all tools</li>
<li>Sample policy matrix and implementation action items</li>
</ul>

<div class="danger-box">
<h4>ðŸš¨ The Shadow IT Problem</h4>
<p>Your company pays for Slack Enterprise and Google Workspace (protected). But employees also use:</p>
<ul>
<li>Personal ChatGPT accounts to draft client emails</li>
<li>Free Claude.ai to review contracts</li>
<li>LinkedIn personal messages to discuss prospects</li>
</ul>
<p><strong>These personal/free accounts don't have enterprise protections.</strong> Company data is leaking into consumer-tier training sets because employees don't realize the distinction.</p>
</div>
</div>

<div id="tab2" class="tab-content">
<h2>Three Data-Use Models You See in the Wild</h2>

<h3>Model 1: Opt-Out ML on De-Identified Data (Traditional Analytics)</h3>

<p><strong>How it works:</strong> Platform uses aggregate, de-identified data to train traditional machine learning models (recommendation engines, search ranking, spam detection). Individual messages or files aren't directly accessed by the models.</p>

<p><strong>Example: Slack's Traditional ML (Pre-AI Features)</strong></p>

<p>Before generative AI features, Slack used de-identified, aggregate data for search relevance and channel recommendations. Their April 2025 clarification states:</p>

<blockquote style="margin:20px 0;padding:20px;background:#f3f4f6;border-left:4px solid #2563eb;font-style:italic">
"Machine learning models are trained on de-identified and aggregated data. They do not access the content of customer messages in DMs, private channels or public channels."
</blockquote>

<p><strong>Risk level: Low.</strong> De-identification and aggregation reduce exposure of specific confidential content. But "what gets discussed" patterns (topics, frequency, user behavior) still inform platform improvements.</p>

<h3>Model 2: Opt-In or Toggle-Based Training on Chats (Generative AI)</h3>

<p><strong>How it works:</strong> Platform offers generative AI features (chatbots, code completion, writing assistants). To power these, the platform trains on actual user inputsâ€”messages, prompts, code. Users can opt in, opt out, or toggle per-workspace.</p>

<p><strong>Example: Anthropic Claude's 2025 Policy Shift</strong></p>

<p>Anthropic (maker of Claude) initially took an "opt-in" approach: by default, they would <em>not</em> use customer data to train models. In 2025, they shifted:</p>

<ul>
<li><strong>Consumer products</strong> (free, Pro, Max): Anthropic will train on chats and coding sessions <em>if users opt in</em>. After a certain date, users must affirmatively choose to opt in or opt out.</li>
<li><strong>Retention extended:</strong> From 30 days to up to <em>five years</em> for safety-classifier training (detecting abuse, harmful content).</li>
<li><strong>Commercial / API products:</strong> Remain excluded from training. Enterprise customers' data is not used to train models.</li>
</ul>

<p><strong>Risk level: Medium-High for consumer users.</strong> If your team uses Claude.ai (free or Pro), be aware that opting in means conversationsâ€”including company strategy, client information, draft contractsâ€”may train future models.</p>

<p><strong>Mitigation:</strong> Use Claude's API or Teams plan for business work, which excludes training. Or configure opt-out settings for consumer accounts.</p>

<h3>Model 3: Ambiguous / Contested Use Leading to Litigation or Backlash</h3>

<p><strong>How it works:</strong> Platform's privacy policy contains vague language ("we may use data to improve services"). Users discover (or fear) that their messages, code, or professional content is being used for AI training. Public backlash or lawsuits force clarification.</p>

<p><strong>Example: LinkedIn Private Messages Controversy (2025)</strong></p>

<p>In 2025, a proposed class action alleged that LinkedIn used private messages of Premium customers to train generative AI models. LinkedIn denied the allegations, produced evidence that private messages weren't used, and the case was voluntarily dismissed.</p>

<p>But the complaint revealed key concerns:</p>

<ul>
<li><strong>Policy ambiguity:</strong> LinkedIn's updated privacy policy included language suggesting data "may be used" for AI, leading users to fear retroactive training.</li>
<li><strong>Non-retroactive opt-outs:</strong> Even where "do not train" settings exist, they often don't apply to data collected <em>before</em> the setting was enabledâ€”raising the question of whether past "confidential" messages were already in the training set.</li>
<li><strong>Reasonable expectation of privacy:</strong> Users argued that labeling messages as "private" created an expectation that they wouldn't be mined for AI. LinkedIn's position was that its terms disclosed potential uses.</li>
</ul>

<p><strong>Risk level: Variable.</strong> When policies are unclear and litigation is ongoing, companies face uncertainty about whether their historical data has already been used for training and whether current opt-outs are effective.</p>
</div>

<div id="tab3" class="tab-content">
<h2>Major Tools: Current AI Training Policies (2025-2026)</h2>

<table>
<thead>
<tr>
<th style="width:15%">Tool</th>
<th style="width:25%">AI Features</th>
<th style="width:30%">Training on User Data?</th>
<th style="width:15%">Opt-Out Available?</th>
<th style="width:15%">Enterprise Carve-Out?</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Slack (Salesforce)</strong></td>
<td>Slack AI (search, summaries, thread insights)</td>
<td><strong>Traditional ML:</strong> De-identified, aggregate data; does not access message content.<br><br><strong>Generative AI:</strong> Uses third-party LLMs (no Slack-trained models on customer data); retrieval-augmented generation only.</td>
<td>N/A â€“ Slack doesn't train generative models on customer data</td>
<td>âœ“ Yes â€“ commercial customers protected by "no customer data used to train LLMs" policy</td>
</tr>
<tr>
<td><strong>Anthropic Claude</strong></td>
<td>Claude chatbot, API, coding assistance</td>
<td><strong>Consumer (free/Pro/Max):</strong> Will train on chats/code sessions if user opts in. Retention up to 5 years for safety classifiers.<br><br><strong>Commercial/API:</strong> Not used for training.</td>
<td>âœ“ Yes â€“ consumer users can opt out (must choose after certain date)</td>
<td>âœ“ Yes â€“ API and Teams plans exclude training</td>
</tr>
<tr>
<td><strong>OpenAI ChatGPT</strong></td>
<td>ChatGPT, API, Codex</td>
<td><strong>Consumer (free/Plus):</strong> By default, conversations may be used for training. Can opt out via settings.<br><br><strong>API / Enterprise:</strong> Not used for training unless explicitly opted in for fine-tuning.</td>
<td>âœ“ Yes â€“ "Data Controls" in settings (consumer); API customers excluded by default</td>
<td>âœ“ Yes â€“ API and Enterprise agreements exclude training</td>
</tr>
<tr>
<td><strong>Google Workspace (Gemini)</strong></td>
<td>Gemini in Gmail, Docs, Sheets; admin AI</td>
<td><strong>Consumer (free Gmail):</strong> May use data for training and ads.<br><br><strong>Workspace (paid):</strong> Google states Workspace data (Docs, Gmail, Drive) is not used to train Gemini or ads models.</td>
<td>Partial â€“ consumer users have limited controls; Workspace customers protected by contract</td>
<td>âœ“ Yes â€“ Workspace terms exclude training on customer content</td>
</tr>
<tr>
<td><strong>Microsoft 365 Copilot</strong></td>
<td>Copilot in Word, Excel, Teams, Outlook</td>
<td><strong>Enterprise:</strong> Microsoft states Copilot does not train on customer data; uses retrieval-augmented generation and enterprise-specific grounding.</td>
<td>N/A â€“ not trained on customer data</td>
<td>âœ“ Yes â€“ enterprise-only feature, protected by DPA</td>
</tr>
<tr>
<td><strong>GitHub Copilot</strong></td>
<td>AI code completion, chat</td>
<td><strong>Suggestions:</strong> Trained on public GitHub repos. Your private code is not used to train Copilot's base models (as of 2023 policy).<br><br><strong>Telemetry:</strong> Usage data (not code content) may inform product improvements.</td>
<td>Partial â€“ can disable telemetry sharing; code content not used for training by policy</td>
<td>âœ“ Yes â€“ GitHub Enterprise Cloud policies exclude training on private repos</td>
</tr>
<tr>
<td><strong>LinkedIn</strong></td>
<td>Feed recommendations, job matching, learning pathways</td>
<td><strong>Traditional ML:</strong> Profile data, connections, activity used for recommendations.<br><br><strong>Generative AI:</strong> Disputed; 2025 lawsuit alleged private messages used for training. LinkedIn denied and case dismissed. "Do not train" setting available but non-retroactive.</td>
<td>âœ“ Yes â€“ "Data for Generative AI Improvement" toggle in settings (added 2024-2025)</td>
<td>Unclear â€“ LinkedIn Sales Navigator and Recruiter may have different terms; check enterprise contracts</td>
</tr>
<tr>
<td><strong>Notion AI</strong></td>
<td>AI writing assistant, Q&A</td>
<td><strong>Notion states:</strong> Customer content is not used to train AI models. AI features use third-party LLMs (OpenAI) under agreements that prohibit training on Notion user data.</td>
<td>N/A â€“ not trained on user data</td>
<td>âœ“ Yes â€“ all plans (including free) protected by same policy</td>
</tr>
<tr>
<td><strong>Zoom AI Companion</strong></td>
<td>Meeting summaries, chat compose</td>
<td><strong>Zoom states:</strong> Customer content (audio, video, chat, files) is not used to train AI models. Uses third-party LLMs with no-training agreements.</td>
<td>N/A â€“ not trained on user data</td>
<td>âœ“ Yes â€“ commercial terms exclude training; admins can disable AI features if desired</td>
</tr>
</tbody>
</table>
</div>

<div id="tab4" class="tab-content">
<h2>Timeline: Recent AI Data Controversies</h2>

<div class="timeline">
<h4>2024-2025: The Year Platforms Clarified (or Muddled) AI Training Policies</h4>

<div class="timeline-item">
<strong>April 2024 â€“ Slack Backlash:</strong> Community outcry over perceived Slack AI training on messages. Salesforce clarified that traditional ML uses de-identified data only and does not access message content. Slack AI (generative) uses third-party LLMs, not Slack-trained models on customer data.
</div>

<div class="timeline-item">
<strong>Fall 2024 â€“ Anthropic Announces Policy Shift:</strong> Anthropic (Claude) announces it will train on consumer chats and coding sessions if users opt in. Previously, default was no training. Retention period extended to up to five years for safety classifiers. Commercial/API customers remain excluded.
</div>

<div class="timeline-item">
<strong>Early 2025 â€“ LinkedIn Lawsuit:</strong> Proposed class action alleges LinkedIn used Premium users' private messages to train generative AI models. LinkedIn denies, produces evidence that private messages weren't used. Case voluntarily dismissed, but highlights user fears about retroactive training and non-retroactive opt-outs.
</div>

<div class="timeline-item">
<strong>April 2025 â€“ Slack Formal Clarification:</strong> Salesforce publishes updated AI privacy principles. Explicitly states: "Machine learning modelsâ€¦ do not access message content" and "no customer data is used to train LLM models." Third-party LLMs used via retrieval-augmented generation.
</div>

<div class="timeline-item">
<strong>May 2025 â€“ Salesforce Blocks Glean and Others:</strong> Reuters reports Salesforce updated Slack ToS to block third-party software (like Glean) from indexing/copying/storing Slack messages long-term, citing AI-era data safeguards.
</div>

<div class="timeline-item">
<strong>January 2026 â€“ Upwork AI Update:</strong> Upwork clarifies that work product and communications data will train AI "exclusively for you" from Jan 5, 2026 onward (prospective only), with double opt-in and AI Preferences controls.
</div>
</div>

<h3>Key Lessons from These Cases</h3>

<h4>Lesson 1: Non-Retroactive Opt-Outs Are the Norm</h4>

<p>Across platforms, when you opt out of AI training, it typically applies only to <em>future</em> data. Content already collected and potentially already used for training stays in the models.</p>

<p><strong>Implications:</strong></p>
<ul>
<li>Configure privacy settings <em>before</em> you share sensitive work, not after</li>
<li>Assume that anything shared before opt-out may already be in a training set</li>
<li>For truly confidential work, don't use tools that ever had training enabledâ€”even if you later opted out</li>
</ul>

<h4>Lesson 2: "Private" Doesn't Mean "Unanalyzed"</h4>

<p>LinkedIn's "private messages" and other platforms' messaging illustrate that "private" often means "only visible to participants," but platforms still reserve the right to analyze them for recommendations, AI, or safety.</p>

<p><strong>Takeaway:</strong> Don't rely on labels. Read the data-use sections of privacy policies to understand what "private" actually protects against.</p>

<h4>Lesson 3: Enterprise Contracts Are Your Best Defense</h4>

<p>Almost every platform that offers enterprise or API tiers excludes training on customer data in those contracts.</p>

<p><strong>Best practice:</strong> For any tool that handles confidential work, use the paid/enterprise version with a DPA or BAA, not the free consumer tier.</p>
</div>

<div id="tab5" class="tab-content">
<h2>Consumer vs Enterprise: The Great Training Divide</h2>

<p>A clear pattern emerges: most platforms treat <strong>consumer/free users</strong> as potential training sources, while <strong>enterprise customers</strong> get contractual protection.</p>

<div class="consumer-enterprise">
<div class="consumer">
<h4>ðŸš¨ Consumer / Free Tier</h4>
<p><strong>Typical terms:</strong></p>
<ul style="font-size:0.95em">
<li>May train on your inputs unless you opt out</li>
<li>Opt-outs often non-retroactive (past data already used)</li>
<li>Retention periods long (months to years)</li>
<li>No contractual guarantee against training</li>
<li>Privacy policy can change with notice</li>
</ul>
<p><strong>Examples:</strong> Free ChatGPT, Claude.ai Free/Pro, consumer Gmail, LinkedIn personal accounts</p>
<p><strong>Risk:</strong> Company work done on personal/free accounts may already be in training sets</p>
</div>

<div class="enterprise">
<h4>âœ“ Enterprise / Paid Business Tier</h4>
<p><strong>Typical terms:</strong></p>
<ul style="font-size:0.95em">
<li>Contractual promise: no training on customer data</li>
<li>Data processing agreements (DPAs)</li>
<li>Admin controls to disable AI features</li>
<li>Compliance certifications (SOC 2, ISO, GDPR)</li>
<li>Shorter retention, clearer deletion rights</li>
</ul>
<p><strong>Examples:</strong> Slack (all paid plans), Claude API/Teams, ChatGPT Enterprise, Google Workspace, Microsoft 365</p>
<p><strong>Protection:</strong> Strong, but verify contract language and periodically audit</p>
</div>
</div>

<div class="warning-box">
<h4>âš  The "Shadow IT" Problem</h4>
<p>Your company pays for Slack Enterprise and Google Workspace (protected). But employees also use:</p>
<ul>
<li>Personal ChatGPT accounts to draft client emails</li>
<li>Free Claude.ai to review contracts</li>
<li>LinkedIn personal messages to discuss prospects</li>
</ul>
<p><strong>These personal/free accounts don't have enterprise protections.</strong> Company data is leaking into consumer-tier training sets because employees don't realize the distinction.</p>
<p><strong>Solution:</strong> Educate teams, provide approved enterprise tools, and ban use of consumer AI for business work in your acceptable use policy.</p>
</div>

<h3>Sample Cross-Platform Policy Matrix</h3>

<table>
<thead>
<tr>
<th style="width:20%">Tool Category</th>
<th style="width:25%">Approved Tools (Enterprise Tier)</th>
<th style="width:25%">Required Settings / Contract Terms</th>
<th style="width:30%">Prohibited for Business Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Team Chat</strong></td>
<td>Slack (paid plans), Microsoft Teams</td>
<td>â€¢ Slack: "No LLM training" per 2025 policy<br>â€¢ Teams: Copilot disabled or enterprise DPA in place</td>
<td>Consumer messaging apps (WhatsApp, Telegram for business without enterprise accounts)</td>
</tr>
<tr>
<td><strong>AI Assistants</strong></td>
<td>ChatGPT Enterprise, Claude API/Teams, Copilot (M365)</td>
<td>â€¢ DPA excluding training on customer data<br>â€¢ Admin controls to audit usage</td>
<td>Free ChatGPT, free Claude.ai, consumer Gemini, Perplexity (for client work)</td>
</tr>
<tr>
<td><strong>Code Repos</strong></td>
<td>GitHub Enterprise, GitLab (paid)</td>
<td>â€¢ Private repos not used for Copilot training<br>â€¢ Telemetry opt-out configured</td>
<td>Public repos for proprietary code; free GitHub without Enterprise terms</td>
</tr>
<tr>
<td><strong>Docs & Knowledge</strong></td>
<td>Google Workspace, Notion (paid), Confluence</td>
<td>â€¢ Workspace: training excluded per terms<br>â€¢ Notion: "no training" policy verified</td>
<td>Consumer Google Docs (free Gmail), public Notion pages for sensitive content</td>
</tr>
<tr>
<td><strong>Freelance Platforms</strong></td>
<td>Upwork (with AI opt-outs configured)</td>
<td>â€¢ AI Preferences: opt out of work product + communications<br>â€¢ Freelancer confirms same settings</td>
<td>PeoplePerHour (messages "not confidential"), Freelancer.com (non-personal UGC) for confidential work</td>
</tr>
<tr>
<td><strong>Professional Networks</strong></td>
<td>LinkedIn (with AI toggle off)</td>
<td>â€¢ "Data for Generative AI Improvement" disabled<br>â€¢ Confidential client discussions banned in LI messages</td>
<td>Using LinkedIn messages for strategy, client names, or deal terms</td>
</tr>
</tbody>
</table>
</div>

<div id="tab6" class="tab-content">
<h2>How This Should Shape Your Company's AI & SaaS Policy</h2>

<p>Your Upwork AI policy is just one piece. A comprehensive company policy should address:</p>

<div class="key-takeaway">
<h4>Minimum Standards for AI & Data Use Across All Tools</h4>

<ol>
<li><strong>No training on confidential/regulated data.</strong> Any tool that handles client information, proprietary methods, PHI, attorney-client communications, or trade secrets must contractually exclude AI training. If it can't, don't use it for that work.</li>

<li><strong>Require enterprise plans or DPAs for business use.</strong> Ban employees from using consumer/free tiers of AI tools (ChatGPT Free, Claude.ai Free, etc.) for company work. Provide approved enterprise accounts instead.</li>

<li><strong>Ban tools that treat UGC as non-personal.</strong> If a platform's ToS says "content you upload is non-personal information not covered by this privacy policy," that's a red flag. Either avoid the tool or strictly limit what you upload.</li>

<li><strong>Add opt-out clauses to vendor contracts.</strong> When negotiating with SaaS vendors, add: "Vendor shall not use Customer Data to train artificial intelligence, machine learning, or automated decision-making models, except as necessary to provide the contracted services exclusively to Customer."</li>

<li><strong>Review policies quarterly.</strong> Assign someone (legal ops, privacy team, or IT security) to track AI policy updates for your tool stack. Set calendar reminders.</li>

<li><strong>Educate employees on consumer vs enterprise tier differences.</strong> Most teams don't know that pasting company strategy into free ChatGPT is different from using ChatGPT Enterprise. Make it explicit in onboarding and acceptable-use training.</li>

<li><strong>Maintain a "shadow IT" watch list.</strong> Tools employees use without IT approval. Periodically survey teams: "What AI or collaboration tools are you using that aren't on the approved list?" Then either approve (with proper contracts) or ban.</li>

<li><strong>Build AI-awareness into vendor due diligence.</strong> When evaluating new SaaS tools, ask: (a) Do you train AI on customer data? (b) If so, can we opt out? (c) Is the opt-out retroactive? (d) Do you share data with third-party AI vendors? (e) Can you provide a DPA excluding AI training?</li>
</ol>
</div>

<h3>Action Items: Building Your Comprehensive AI & SaaS Policy</h3>

<ol>
<li><strong>Inventory your current tool stack.</strong> List every SaaS tool teams use for communication, collaboration, AI assistance, and project management.</li>

<li><strong>Classify by data sensitivity.</strong> Tag each tool: Does it handle confidential client data? Proprietary code? Regulated information (HIPAA, attorney-client)? Or only public/internal non-sensitive content?</li>

<li><strong>Research current AI policies for each tool.</strong> Use the table in this article as a starting point. Visit each vendor's privacy policy, AI documentation, and enterprise terms.</li>

<li><strong>Map consumer vs enterprise accounts.</strong> For every tool, identify: Are we using free/consumer tier or paid/enterprise? If consumer, is there an enterprise option that excludes training?</li>

<li><strong>Upgrade critical tools to enterprise tier.</strong> Prioritize tools handling confidential data. Budget for ChatGPT Enterprise, Claude Teams, Slack paid plans, etc.</li>

<li><strong>Configure opt-outs where available.</strong> For tools that offer AI training toggles (Upwork, LinkedIn, etc.), set company standard: opt out by default.</li>

<li><strong>Draft vendor contract addendum.</strong> Standard language requiring vendors to exclude AI training. Attach to all new SaaS agreements.</li>

<li><strong>Update acceptable use policy.</strong> Add section: "Employees may not use consumer/free AI tools (ChatGPT Free, Claude.ai Free, etc.) for company business. Use only approved enterprise accounts listed in [internal wiki]."</li>

<li><strong>Train employees.</strong> One-page guide: "Which AI tools can I use for client work? What settings do I need to configure?" Include screenshots.</li>

<li><strong>Schedule quarterly reviews.</strong> Every Q, re-check: (a) Have vendor policies changed? (b) Are we still using approved tools correctly? (c) Any shadow IT to address?</li>
</ol>

<h3>Connecting Back to Upwork and Freelance Marketplaces</h3>

<p>Your "Upwork policy" should be <em>consistent</em> with your "Slack policy" and your "Claude policy."</p>

<p><strong>Example inconsistency to avoid:</strong></p>

<ul>
<li>You ban AI training on Upwork (configure AI Preferences to opt out)</li>
<li>But your team uses free ChatGPT to draft proposals for Upwork jobs</li>
<li>And discusses Upwork projects in personal LinkedIn messages</li>
</ul>

<p><strong>Result:</strong> Data you protected on one platform leaks through others.</p>

<p><strong>Better approach:</strong></p>

<ul>
<li><strong>Upwork:</strong> AI Preferences set to opt out of work product and communications training</li>
<li><strong>ChatGPT:</strong> Enterprise account with no-training DPA, or ban use of ChatGPT for client work entirely</li>
<li><strong>LinkedIn:</strong> "Data for Generative AI Improvement" toggled off; ban discussion of confidential client details in LinkedIn messages; use encrypted email for business development</li>
<li><strong>Slack:</strong> Paid plan (protected by "no LLM training" policy); admin controls set to minimize data retention</li>
</ul>

<p>This creates a consistent data perimeter.</p>
</div>

<div id="tab7" class="tab-content">
<h2>Attorney Services: AI Data Privacy & SaaS Policy Consulting</h2>

<p>Most companies discover their AI training exposure <em>after</em> they've already used dozens of tools with inadequate protections. By then, confidential data may already be in training setsâ€”and retroactive opt-outs won't remove it.</p>

<p>I help businesses build comprehensive, enforceable AI and SaaS data policies <em>before</em> exposure occursâ€”and remediate exposure when it's already happened.</p>

<h3>How I Can Help</h3>

<div class="info-box">
<strong>Services for Companies:</strong>
<ul style="margin:10px 0 0 20px">
<li><strong>AI & SaaS Policy Audit:</strong> I inventory your current tool stack, identify AI training exposure, and map consumer vs enterprise tier usage across your organization</li>
<li><strong>Policy Development:</strong> I draft company-wide AI acceptable use policies, vendor contract addenda, and employee training materials</li>
<li><strong>Vendor Negotiation:</strong> I negotiate DPAs and AI opt-out terms with SaaS vendors on your behalf</li>
<li><strong>Incident Response:</strong> When you discover that confidential data was used for AI training without authorization, I advise on remediation, demand letters to vendors, and potential litigation</li>
<li><strong>Ongoing Compliance:</strong> Quarterly policy reviews to track vendor AI policy changes and update your internal controls</li>
</ul>
</div>

<div class="info-box">
<strong>Services for Individuals & Freelancers:</strong>
<ul style="margin:10px 0 0 20px">
<li><strong>Platform Policy Analysis:</strong> I review Upwork, Fiverr, Freelancer.com, and other marketplace AI policies and configure your accounts for maximum privacy protection</li>
<li><strong>NDA & Contract Review:</strong> I draft or review freelancer agreements with AI training opt-out clauses and intellectual property protections</li>
<li><strong>Dispute Resolution:</strong> When platforms train on your work despite contractual prohibitions, I send demand letters and pursue remedies</li>
</ul>
</div>

<h3>Why Specialized Counsel Matters</h3>

<p>AI data training policies are evolving monthly. Generic business attorneys often lack the specific knowledge required:</p>

<ul>
<li><strong>Platform-specific expertise:</strong> Understanding how Slack, Claude, Upwork, LinkedIn, and others actually use dataâ€”not just what their privacy policies say</li>
<li><strong>Technical fluency:</strong> Knowing the difference between de-identified ML training, RAG (retrieval-augmented generation), and full model training on customer data</li>
<li><strong>Cross-platform policy design:</strong> Building policies that protect data consistently across 10-20 different tools</li>
<li><strong>Remediation strategies:</strong> What to do when data is already compromised and retroactive opt-outs don't help</li>
</ul>

<div class="cta-box">
<h3 style="margin-top:0">Schedule an AI Data Privacy Consultation</h3>
<p>Whether you're building an AI policy from scratch, responding to an exposure incident, or negotiating with vendors, I provide practical, enforceable solutions that protect your data across all platforms.</p>

<p>Send me your current tool stack list, any vendor contracts you have concerns about, and a summary of what types of data you need to protect. I'll evaluate your exposure and outline a policy and remediation strategy.</p>

<!-- Calendly inline widget begin -->
<div class="calendly-inline-widget" data-url="https://calendly.com/sergei-tokmakov/30-minute-zoom-meeting?hide_gdpr_banner=1" style="min-width:320px;height:700px;"></div>
<script type="text/javascript" src="https://assets.calendly.com/assets/external/widget.js" async></script>
<!-- Calendly inline widget end -->

<p style="margin-top:20px;font-size:0.95em">Email: <strong><a href="mailto:owner@terms.law" style="color:#dc2626">owner@terms.law</a></strong></p>
<p style="font-size:0.95em">AI policy audits: $400-$900. Contract drafting: ~$450 (typically 2 hours @ $240/hr). Vendor negotiation: $240/hr. Incident response: hourly or contingency arrangements available.</p>
</div>

</div>

</div>

<script>
function openTab(evt,tabName){const tabcontent=document.getElementsByClassName("tab-content");for(let i=0;i<tabcontent.length;i++){tabcontent[i].classList.remove("active")}const tabs=document.getElementsByClassName("tab");for(let i=0;i<tabs.length;i++){tabs[i].classList.remove("active")}document.getElementById(tabName).classList.add("active");evt.currentTarget.classList.add("active")}
</script>
</body>
</html>
